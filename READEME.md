# ğŸ§  PacMan Rede Neural ğŸ®
Este projeto Ã© um experimento de InteligÃªncia Artificial com Aprendizado Evolutivo, onde uma rede neural Ã© treinada para jogar Pac-Man de forma autÃ´noma. A IA aprende por meio de um algoritmo genÃ©tico, aprimorando suas decisÃµes a cada geraÃ§Ã£o.

## ğŸš€ VisÃ£o Geral
Neste sistema, agentes controlados por uma rede neural enfrentam o desafio de jogar Pac-Man. Cada agente decide entre quatro possÃ­veis movimentos:

- â¬†ï¸ Cima

- â¬‡ï¸ Baixo

- â¬…ï¸ Esquerda

- â¡ï¸ Direita

Ao longo de vÃ¡rias geraÃ§Ãµes, esses agentes evoluem com base em seu desempenho no jogo, usando conceitos de seleÃ§Ã£o natural, elitismo, crossover e mutaÃ§Ã£o genÃ©tica.

## ğŸ§¬ Arquitetura do Projeto
### ğŸ§  Rede Neural
A rede neural Ã© responsÃ¡vel por tomar decisÃµes de movimento com base no estado atual do jogo. Ela pode receber informaÃ§Ãµes como:

- PosiÃ§Ã£o do Pac-Man

- PosiÃ§Ã£o dos itens a serem coletados

- DistÃ¢ncia para o objetivo

- PresenÃ§a de paredes ou obstÃ¡culos

- HistÃ³rico de movimentos anteriores

### âš™ï¸ SaÃ­das da Rede
A rede retorna 4 saÃ­das numÃ©ricas (uma para cada direÃ§Ã£o). O movimento escolhido Ã© o que tem a maior ativaÃ§Ã£o dentre essas saÃ­das.

### ğŸ§ª Aprendizado por EvoluÃ§Ã£o
O projeto utiliza um algoritmo genÃ©tico, onde populaÃ§Ãµes de redes neurais evoluem de geraÃ§Ã£o em geraÃ§Ã£o. A evoluÃ§Ã£o segue os seguintes princÃ­pios:

### ğŸŒŸ Elitismo
Os melhores agentes da geraÃ§Ã£o atual sÃ£o copiados diretamente para a prÃ³xima geraÃ§Ã£o sem alteraÃ§Ãµes, garantindo que as boas estratÃ©gias nÃ£o se percam.

### ğŸ’ Crossover
Pares de agentes combinam seus â€œgenesâ€ (pesos da rede neural) para gerar novos agentes com caracterÃ­sticas de ambos.

### âš¡ MutaÃ§Ã£o
ApÃ³s o crossover, uma pequena parte dos genes pode ser modificada aleatoriamente, permitindo a exploraÃ§Ã£o de novas estratÃ©gias.

### ğŸ“ˆ Sistema de PontuaÃ§Ã£o
A pontuaÃ§Ã£o de cada agente Ã© calculada com base em:

- ğŸ’ Coleta de itens â†’ Recompensa positiva

- ğŸ¯ Proximidade ao objetivo â†’ Recompensa baseada na reduÃ§Ã£o da distÃ¢ncia

- ğŸ” Movimentos em loop â†’ Penalidade para evitar padrÃµes inÃºteis

- ğŸ•’ Tempo parado ou ineficiente â†’ Penalidade para evitar estagnaÃ§Ã£o

Essas regras guiam o processo evolutivo, selecionando os agentes mais eficazes no ambiente.

## ğŸ“Š Registro de EvoluÃ§Ã£o
O sistema acompanha os seguintes indicadores durante o treinamento:

- ğŸ§¬ GeraÃ§Ã£o Atual

- ğŸ† Melhor PontuaÃ§Ã£o da GeraÃ§Ã£o

- ğŸ¥‡ Melhor PontuaÃ§Ã£o Geral

- ğŸ“‰ Progresso ao Longo do Tempo

Esses dados ajudam a monitorar a performance da IA e avaliar a eficiÃªncia da evoluÃ§Ã£o.

## ğŸ› ï¸ Como Funciona
- Uma populaÃ§Ã£o inicial de agentes Ã© gerada com redes neurais aleatÃ³rias.

- Cada agente joga Pac-Man por um tempo limitado.

- Ao final, os agentes sÃ£o avaliados e ranqueados por desempenho.

- A nova geraÃ§Ã£o Ã© criada a partir dos melhores agentes via elitismo, crossover e mutaÃ§Ã£o.

- O ciclo se repete, aprimorando a habilidade dos agentes a cada geraÃ§Ã£o.

## ğŸ¯ Objetivo
Criar uma IA capaz de jogar Pac-Man de forma eficaz apenas por meio de tentativa, erro e evoluÃ§Ã£o, sem qualquer conhecimento prÃ©vio do jogo ou aprendizado supervisionado.